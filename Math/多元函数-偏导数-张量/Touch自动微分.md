![image-20250112183210231](assets\image-20250112183210231.png)





![image-20250112183312170](assets\image-20250112183312170.png)



![image-20250112183341716](assets\image-20250112183341716.png)



![image-20250112183420801](assets\image-20250112183420801.png)





![image-20250112183604661](assets\image-20250112183604661.png)



![image-20250112183736889](assets\image-20250112183736889.png)

# 自动微分程序设计

## 手动微分与自动微分的对比

### 手动微分

**手动微分**是指开发者根据数学公式，手动计算出目标函数关于各个变量的导数。这个过程虽然在简单函数中比较直观，但在复杂模型（如深度神经网络）中，手动计算导数不仅繁琐，而且容易出错。

**优点：**

- 对基本函数的微分有深刻理解。
- 在简单场景下，计算速度快。

**缺点：**

- 随着模型复杂度增加，计算工作量呈指数级增长。
- 容易出错，尤其是在复杂的计算图中。
- 缺乏灵活性，难以适应动态变化的模型结构。

### 自动微分

**自动微分**（Automatic Differentiation, AD）是一种通过程序自动计算函数导数和梯度的技术。它通过解析计算图，系统地应用链式法则，确保梯度计算的准确性和高效性。

**优点：**

- **高效性**：能够处理复杂的计算图，计算速度快。
- **准确性**：减少人为计算错误，确保梯度精度。
- **便利性**：简化模型开发过程，开发者无需手动推导导数。

**缺点：**

- 需要额外的内存来存储计算图。
- 对于某些特殊情况，可能需要调整计算图结构。

### 对比总结

| 特性           | 手动微分                       | 自动微分                           |
| -------------- | ------------------------------ | ---------------------------------- |
| **计算复杂度** | 低到中等，取决于函数复杂度     | 高效，适用于复杂计算图             |
| **准确性**     | 依赖开发者的推导和实现         | 高，基于系统化的链式法则应用       |
| **灵活性**     | 低，难以适应动态变化的模型结构 | 高，适用于各种复杂和动态的模型结构 |
| **开发效率**   | 低，手动推导耗时且易出错       | 高，自动处理梯度计算过程           |

通过对比，可以看出自动微分在处理复杂函数和模型时具有显著优势，尤其在深度学习等领域，极大地提升了开发效率和计算准确性。

## 定义并计算二次函数及其导数

本节将通过手动微分和自动微分两种方式，定义一个简单的二次函数 $ f(x) = x^2 + 3x + 2 $ 及其导数 $ f'(x) = 2x + 3 $，并计算其值和导数。

### 手动微分示例

首先，我们通过手动定义函数及其导数，并计算它们的值。

```python
import matplotlib.pyplot as plt
import numpy as np

def f(x):
    return x**2 + 3*x + 2

def df(x):
    return 2*x + 3

# 生成自变量x的值
x = np.linspace(-10, 10, 100)
yf = f(x)
y_df = df(x)

# 绘制图像
plt.plot(x, yf, label='f(x) = x^2 + 3x + 2', color='blue')
plt.plot(x, y_df, label="f'(x) = 2x + 3", color='orange')
plt.grid(True)
plt.legend()
plt.xlabel('x')
plt.ylabel('y')
plt.title('函数及其导数图像')
plt.show()
```

#### 代码解析

1. **导入库**
   - `matplotlib.pyplot`：用于绘图。
   - `numpy`：用于生成数值数据。

2. **定义函数**
   - `f(x)`：计算 $ f(x) = x^2 + 3x + 2 $。
   - `df(x)`：计算 $ f'(x) = 2x + 3 $。

3. **生成数据**
   - 使用 `np.linspace` 生成自变量 $ x $ 的值，范围为 -10 到 10，共100个点。
   - 计算函数值 `yf` 和导数值 `y_df`。

4. **绘制图像**
   - 使用 `plt.plot` 绘制函数 $ f(x) $ 和导数 $ f'(x) $ 的图像。
   - 添加网格线 `plt.grid(True)`。
   - 显示图例 `plt.legend()`。
   - 添加坐标轴标签和标题。
   - 展示图像 `plt.show()`。

#### 运行结果

运行上述程序后，可以看到两条曲线：

- **蓝色抛物线**：对应函数 $ f(x) $。
- **橙色直线**：对应导函数 $ f'(x) $。

![函数及其导数图像](https://i.imgur.com/your_image_link.png)  
*图1：函数 $ f(x) $ 及其导数 $ f'(x) $ 的图像*

### 自动微分示例

接下来，我们使用拍Touch框架的自动微分功能，重新编写上述代码，通过自动微分计算梯度。

```python
import touch
import matplotlib.pyplot as plt

# 定义函数 f(x) = x^2 + 3x + 2
def f(x):
    return x**2 + 3*x + 2

# 生成自变量x的值，并启用自动微分
x = touch.linspace(-10, 10, 100, requires_grad=True)
yf = f(x)

# 计算梯度
yf.sum().backward()
y_df = x.grad

# 将张量转为numpy数组
x_np = x.detach().numpy()
yf_np = yf.detach().numpy()
y_df_np = y_df.detach().numpy()

# 绘制图像
plt.plot(x_np, yf_np, label='f(x) = x^2 + 3x + 2', color='blue')
plt.plot(x_np, y_df_np, label="f'(x) = 2x + 3", color='orange')
plt.grid(True)
plt.legend()
plt.xlabel('x')
plt.ylabel('y')
plt.title('函数及其导数图像（自动微分）')
plt.show()
```

#### 代码解析

1. **导入库**
   - `touch`：拍Touch框架，用于自动微分。
   - `matplotlib.pyplot`：用于绘图。

2. **定义函数**
   - `f(x)`：计算 $ f(x) = x^2 + 3x + 2 $。

3. **生成数据并启用自动微分**
   - 使用 `touch.linspace` 生成自变量 $ x $ 的值，范围为 -10 到 10，共100个点。
   - 参数 `requires_grad=True` 表示启用自动微分功能，记录计算图。
   - 计算函数值 `yf = f(x)`。

4. **计算梯度**
   - 调用 `yf.sum().backward()` 计算 `yf` 关于 `x` 的梯度。
   - `yf.sum()` 将所有元素求和，生成一个标量，以满足 `backward()` 的要求。
   - 梯度值保存在 `x.grad` 中，即 `y_df = x.grad`。

5. **数据转换**
   - 使用 `detach()` 方法创建与计算图无关的张量副本，避免梯度继续传播。
   - 使用 `numpy()` 方法将张量转换为 NumPy 数组，便于绘图。

6. **绘制图像**
   - 绘制函数 $ f(x) $ 和导数 $ f'(x) $ 的图像。
   - 添加网格线、图例、坐标轴标签和标题。
   - 展示图像 `plt.show()`。

#### 运行结果

运行上述程序后，绘制出的图像与手动微分的结果相同：

- **蓝色抛物线**：对应函数 $ f(x) $。
- **橙色直线**：对应导函数 $ f'(x) $。

![函数及其导数图像（自动微分）](https://i.imgur.com/your_image_link.png)  
*图2：函数 $ f(x) $ 及其导数 $ f'(x) $ 的图像（自动微分）*

## 绘制函数及其导数的图像

通过手动微分和自动微分的代码示例，我们能够直观地看到函数及其导数的形状。这有助于理解函数的增长趋势及其导数在不同点的变化情况。例如，函数 $ f(x) = x^2 + 3x + 2 $ 是一个开口向上的抛物线，其导数 $ f'(x) = 2x + 3 $ 是一条直线，表示函数在不同点的斜率。

![函数及其导数图像对比](https://i.imgur.com/your_image_link.png)  
*图3：手动微分与自动微分的函数及其导数图像对比*

## 深入理解自动微分的内部机制

### 自动微分的基本原理

自动微分通过构建计算图（Computational Graph），系统地记录每一步计算操作，并基于链式法则自动计算导数。计算图分为**前向传播**和**反向传播**两个阶段：

1. **前向传播（Forward Pass）**：计算函数的输出，同时记录计算过程中的操作和中间变量。
2. **反向传播（Backward Pass）**：根据计算图，按照链式法则从输出开始，逐步计算各个变量的梯度。

### 计算图的构建

以函数 $ f(x) = x^2 + 3x + 2 $ 为例，计算图如下：

```
x
│
├─> Square (x^2)
│
├─> Multiply (3x)
│
└─> Add (x^2 + 3x + 2) = f(x)
```

在反向传播阶段，自动微分将逐步计算每个节点的梯度：

1. **Add节点**：$ \frac{\partial f}{\partial x^2} = 1 $, $ \frac{\partial f}{\partial 3x} = 1 $
2. **Square节点**：$ \frac{\partial f}{\partial x} = \frac{\partial f}{\partial x^2} \cdot \frac{\partial x^2}{\partial x} = 1 \cdot 2x $
3. **Multiply节点**：$ \frac{\partial f}{\partial x} += \frac{\partial f}{\partial 3x} \cdot \frac{\partial 3x}{\partial x} = 1 \cdot 3 $

最终，合并所有梯度得到 $ f'(x) = 2x + 3 $。

### 反向传播算法

反向传播是自动微分中核心的梯度计算方法。其基本步骤如下：

1. **前向传播**：
   - 计算函数的输出值，同时构建计算图，记录每一步操作和中间变量。

2. **反向传播**：
   - 从输出节点开始，按照计算图的拓扑顺序，应用链式法则计算各个节点的梯度。
   - 将梯度逐层传递回输入节点，最终得到输入变量的梯度。

### 自动微分的实现方式

自动微分主要有两种实现方式：

1. **正向模式自动微分（Forward Mode AD）**：
   - 适合于输入变量较少、输出变量较多的场景。
   - 计算每个输入变量的导数。

2. **反向模式自动微分（Reverse Mode AD）**：
   - 适合于输入变量较多、输出变量较少（通常为标量）的场景，如神经网络训练。
   - 先进行前向传播计算函数值，再通过反向传播计算梯度。

在深度学习框架中，通常采用反向模式自动微分，因为它在计算多输入单输出函数的梯度时更加高效。

## 验证自动微分结果

为了确保自动微分计算的梯度值的准确性，我们可以通过手动计算导数来验证自动微分的结果是否正确。

### 验证示例代码

```python
import touch

# 定义函数 f(x) = x^2 - 4x - 5
def f(x):
    return x**2 - 4*x - 5

# 定义导函数 f'(x) = 2x - 4
def df(x):
    return 2*x - 4

# 初始化张量x，启用自动微分
x = touch.tensor(2.0, requires_grad=True)

# 计算函数值 y
y = f(x)

# 计算梯度
y.backward()
print(f"x = {x.data}, grad = {x.grad.data}")  # 预期输出: x = 2.0, grad = 0.0

# 使用自定义导函数验证梯度
print(f"df(x) = {df(x.data)}")  # 预期输出: df(x) = 0.0

# 清零梯度
x.grad.zero_()

# 重新计算y和梯度
y = f(x)
y.backward()
print(f"x = {x.data}, grad = {x.grad.data}")  # 预期输出: x = 2.0, grad = 0.0

# 再次验证导数
print(f"df(x) = {df(x.data)}")  # 预期输出: df(x) = 0.0
```

#### 代码解析

1. **定义函数及导函数**
   - `f(x)`：定义 $ f(x) = x^2 - 4x - 5 $。
   - `df(x)`：定义导函数 $ f'(x) = 2x - 4 $。

2. **初始化张量**
   - 使用 `touch.tensor` 初始化张量 `x`，值为 2.0，并启用自动微分 `requires_grad=True`。

3. **计算函数值和梯度**
   - 计算 `y = f(x)`。
   - 调用 `y.backward()` 计算梯度 $ \frac{dy}{dx} $。
   - 输出 `x` 的值和梯度：`x = 2.0, grad = 0.0`。

4. **验证导数**
   - 使用定义的导函数 `df(x)` 验证梯度值，结果应为 `df(x) = 0.0`，与自动微分结果一致。

5. **清零梯度**
   - 使用 `x.grad.zero_()` 将梯度清零，为下一次计算做准备。

6. **重新计算函数值和梯度**
   - 重新计算 `y = f(x)` 并调用 `y.backward()`。
   - 再次输出 `x` 的值和梯度，结果应保持一致：`x = 2.0, grad = 0.0`。

#### 运行结果

```
x = 2.0, grad = 0.0
df(x) = 0.0
x = 2.0, grad = 0.0
df(x) = 0.0
```

结果表明，自动微分计算的梯度与手动计算的导数完全一致，验证了自动微分的准确性。

### 注意事项

- **梯度累加**：每次调用 `backward()` 时，梯度会累加到已有的梯度上。因此，如果不清零梯度，可能会导致梯度错误。例如，第一次计算梯度为 -4，第二次再计算时梯度会变为 -8。
- **清零梯度**：在重新计算梯度前，应调用 `x.grad.zero_()` 清空之前的梯度，以确保计算结果的正确性。

## 实现梯度下降算法

通过自动微分，我们可以轻松地实现梯度下降算法，以求解函数的极值点。下面，我们将使用自动微分实现梯度下降算法，寻找函数 $ f(x) = x^2 - 4x - 5 $ 的极值点。

### 手动微分实现梯度下降

首先，通过手动微分计算梯度，并实现梯度下降算法。

```python
import matplotlib.pyplot as plt
import numpy as np

# 定义函数 f(x) = x^2 - 4x - 5
def f(x):
    return x**2 - 4*x - 5

# 定义导函数 f'(x) = 2x - 4
def df(x):
    return 2*x - 4

# 初始化x
x = 0.0

# 设置迭代次数和学习率
num_iterations = 100
learning_rate = 0.1

for i in range(num_iterations):
    y = f(x)        # 计算函数值
    grad = df(x)    # 计算导数
    
    # 更新x的值
    x -= learning_rate * grad

print(f"极值点 x = {x}, f(x) = {f(x)}")
```

#### 代码解析

1. **导入库**
   - `matplotlib.pyplot` 和 `numpy`（未在本代码中使用，但通常用于可视化和数值计算）。

2. **定义函数及导函数**
   - `f(x)`：定义 $ f(x) = x^2 - 4x - 5 $。
   - `df(x)`：定义导函数 $ f'(x) = 2x - 4 $。

3. **初始化参数**
   - 直接使用浮点数 `x = 0.0` 作为初始值。

4. **设置超参数**
   - 迭代次数 `num_iterations`：100 次。
   - 学习率 `learning_rate`：0.1。

5. **梯度下降循环**
   - **计算函数值**：`y = f(x)`。
   - **计算导数**：`grad = df(x)`。
   - **更新参数**：`x -= learning_rate * grad`。

6. **输出结果**
   - 打印极值点 `x` 和函数值 `f(x)`。

#### 运行结果

```
极值点 x = 2.0, f(x) = -9.0
```

这表明函数 $ f(x) = x^2 - 4x - 5 $ 在 $ x = 2 $ 处取得极小值，函数值为 -9。

### 自动微分实现梯度下降

使用自动微分功能，结合优化器，实现梯度下降算法。

```python
import touch

# 定义函数 f(x) = x^2 - 4x - 5
def f(x):
    return x**2 - 4*x - 5

# 初始化张量x，启用自动微分
x = touch.tensor(0.0, requires_grad=True)

# 定义优化器
optimizer = touch.optim.SGD([x], lr=0.1)

# 设置迭代次数
num_iterations = 100

for i in range(num_iterations):
    optimizer.zero_grad()    # 清零梯度
    y = f(x)                 # 计算函数值
    y.backward()             # 计算梯度
    optimizer.step()         # 更新参数

print(f"极值点 x = {x.data}, f(x) = {f(x.data)}")
```

#### 代码解析

1. **导入库**
   - `touch`：拍Touch框架，用于自动微分和优化器。

2. **定义函数**
   - `f(x)`：计算 $ f(x) = x^2 - 4x - 5 $。

3. **初始化张量**
   - 使用 `touch.tensor` 初始化张量 `x`，初始值为 0.0，启用自动微分 `requires_grad=True`。

4. **定义优化器**
   - 使用 `touch.optim.SGD` 定义随机梯度下降优化器，传入参数 `x` 和学习率 `lr=0.1`。

5. **设置超参数**
   - 迭代次数 `num_iterations`：100 次。

6. **梯度下降循环**
   - **清零梯度**：调用 `optimizer.zero_grad()` 清空之前的梯度。
   - **计算函数值**：`y = f(x)`。
   - **计算梯度**：调用 `y.backward()`，梯度保存在 `x.grad` 中。
   - **更新参数**：调用 `optimizer.step()` 根据梯度更新参数 `x`。

7. **输出结果**
   - 打印极值点 `x.data` 和函数值 `f(x.data)`。

#### 运行结果

```
极值点 x = 2.0, f(x) = -9.0
```

结果与手动微分实现一致，表明自动微分方法的有效性。

### 详细对比分析

| 步骤           | 手动微分实现梯度下降                           | 自动微分实现梯度下降                                         |
| -------------- | ---------------------------------------------- | ------------------------------------------------------------ |
| **导入库**     | 仅导入 `matplotlib` 和 `numpy`（未使用）       | 导入 `touch` 框架及其优化器模块                              |
| **定义函数**   | 定义 `f(x)` 和 `df(x)` 两个函数                | 定义 `f(x)`，无需显式定义 `df(x)`                            |
| **初始化参数** | 直接使用浮点数 `x = 0.0`                       | 使用 `touch.tensor` 初始化张量 `x`，并启用自动微分 `requires_grad=True` |
| **设置超参数** | 设置迭代次数和学习率                           | 设置迭代次数和学习率，并定义优化器 `optimizer = touch.optim.SGD([x], lr=0.1)` |
| **梯度计算**   | 手动调用 `df(x)` 计算导数                      | 调用 `y.backward()` 自动计算梯度                             |
| **参数更新**   | 直接修改 `x` 的值：`x -= learning_rate * grad` | 使用优化器 `optimizer.step()` 自动更新参数                   |
| **梯度清零**   | 无需手动清零梯度                               | 使用 `optimizer.zero_grad()` 清零梯度                        |
| **输出结果**   | 直接打印 `x` 和 `f(x)` 的值                    | 打印 `x.data` 和 `f(x.data)` 的值                            |
| **代码简洁性** | 需要显式定义和调用导数函数，代码较为冗长       | 自动处理梯度计算和参数更新，代码更加简洁和规范               |
| **扩展性**     | 难以扩展到多变量和复杂模型                     | 易于扩展到多变量和复杂模型，适用于深度学习等复杂场景         |
| **错误易发性** | 高，手动计算导数易出错                         | 低，自动微分系统化处理梯度计算，减少人为错误                 |
| **灵活性**     | 低，难以适应动态变化的模型结构                 | 高，支持动态图和复杂计算图，适应性强                         |

**总结**：

- **代码简洁**：自动微分减少了显式定义导数函数的需求，代码更加简洁易读。
- **准确性高**：自动微分基于计算图和链式法则，确保梯度计算的准确性，避免了手动计算中的错误。
- **扩展性强**：适用于多变量和复杂模型，易于扩展到深度学习等复杂应用场景。
- **开发效率高**：自动处理梯度计算和参数更新，开发者可以更专注于模型设计和优化。

## 自动微分与手动微分的详细对比

为了更深入地理解自动微分的优势，我们将手动微分和自动微分在实现梯度下降算法时的具体步骤进行详细对比。

### 手动微分实现梯度下降

```python
import matplotlib.pyplot as plt
import numpy as np

# 定义函数 f(x) = x^2 - 4x - 5
def f(x):
    return x**2 - 4*x - 5

# 定义导函数 f'(x) = 2x - 4
def df(x):
    return 2*x - 4

# 初始化x
x = 0.0

# 设置迭代次数和学习率
num_iterations = 100
learning_rate = 0.1

for i in range(num_iterations):
    y = f(x)        # 计算函数值
    grad = df(x)    # 计算导数
    
    # 更新x的值
    x -= learning_rate * grad

print(f"极值点 x = {x}, f(x) = {f(x)}")
```

### 自动微分实现梯度下降

```python
import touch

# 定义函数 f(x) = x^2 - 4x - 5
def f(x):
    return x**2 - 4*x - 5

# 初始化张量x，启用自动微分
x = touch.tensor(0.0, requires_grad=True)

# 定义优化器
optimizer = touch.optim.SGD([x], lr=0.1)

# 设置迭代次数
num_iterations = 100

for i in range(num_iterations):
    optimizer.zero_grad()    # 清零梯度
    y = f(x)                 # 计算函数值
    y.backward()             # 计算梯度
    optimizer.step()         # 更新参数

print(f"极值点 x = {x.data}, f(x) = {f(x.data)}")
```

### 详细对比分析

| 步骤           | 手动微分实现梯度下降                           | 自动微分实现梯度下降                                         |
| -------------- | ---------------------------------------------- | ------------------------------------------------------------ |
| **导入库**     | 仅导入 `matplotlib` 和 `numpy`（未使用）       | 导入 `touch` 框架及其优化器模块                              |
| **定义函数**   | 定义 `f(x)` 和 `df(x)` 两个函数                | 定义 `f(x)`，无需显式定义 `df(x)`                            |
| **初始化参数** | 直接使用浮点数 `x = 0.0`                       | 使用 `touch.tensor` 初始化张量 `x`，并启用自动微分 `requires_grad=True` |
| **设置超参数** | 设置迭代次数和学习率                           | 设置迭代次数和学习率，并定义优化器 `optimizer = touch.optim.SGD([x], lr=0.1)` |
| **梯度计算**   | 手动调用 `df(x)` 计算导数                      | 调用 `y.backward()` 自动计算梯度                             |
| **参数更新**   | 直接修改 `x` 的值：`x -= learning_rate * grad` | 使用优化器 `optimizer.step()` 自动更新参数                   |
| **梯度清零**   | 无需手动清零梯度                               | 使用 `optimizer.zero_grad()` 清零梯度                        |
| **输出结果**   | 直接打印 `x` 和 `f(x)` 的值                    | 打印 `x.data` 和 `f(x.data)` 的值                            |
| **代码简洁性** | 需要显式定义和调用导数函数，代码较为冗长       | 自动处理梯度计算和参数更新，代码更加简洁和规范               |
| **扩展性**     | 难以扩展到多变量和复杂模型                     | 易于扩展到多变量和复杂模型，适用于深度学习等复杂场景         |
| **错误易发性** | 高，手动计算导数易出错                         | 低，自动微分系统化处理梯度计算，减少人为错误                 |
| **灵活性**     | 低，难以适应动态变化的模型结构                 | 高，支持动态图和复杂计算图，适应性强                         |

**总结**：

通过对比可以看出，自动微分在实现梯度下降算法时具有显著的优势：

- **代码简洁**：自动微分减少了显式定义导数函数的需求，代码更加简洁易读。
- **准确性高**：自动微分基于计算图和链式法则，确保梯度计算的准确性，避免了手动计算中的错误。
- **扩展性强**：适用于多变量和复杂模型，易于扩展到深度学习等复杂应用场景。
- **开发效率高**：自动处理梯度计算和参数更新，开发者可以更专注于模型设计和优化。

## 自动微分的内部机制与理解

### 计算图的构建与管理

自动微分依赖于计算图来记录计算过程中的操作和变量。每一个操作（如加法、乘法、激活函数等）都在计算图中表示为一个节点，节点之间的连接表示数据依赖关系。在反向传播阶段，自动微分通过遍历计算图，从输出节点开始，逐步计算各个节点的梯度。

### 反向传播算法

反向传播是自动微分中核心的梯度计算方法。其基本步骤如下：

1. **前向传播**：
   - 计算函数的输出值，同时构建计算图，记录每一步操作和中间变量。

2. **反向传播**：
   - 从输出节点开始，按照计算图的拓扑顺序，应用链式法则计算各个节点的梯度。
   - 将梯度逐层传递回输入节点，最终得到输入变量的梯度。

### 自动微分的实现细节

以拍Touch（类似PyTorch）为例，自动微分的实现包括以下关键步骤：

1. **张量（Tensor）的定义**：
   - 张量是自动微分的基本数据结构，包含数据值、梯度信息和计算图信息。
   - 启用自动微分的张量会记录参与计算的操作，以便在反向传播时构建计算图。

2. **操作符重载**：
   - 自动微分框架通过重载Python的运算符，实现对张量操作的拦截和记录。
   - 每一个张量操作都会生成一个新的计算图节点，记录操作类型和输入输出关系。

3. **计算图的存储**：
   - 计算图通常以有向无环图（DAG）的形式存储，确保在反向传播时能够按照正确的顺序计算梯度。
   - 每个张量对象包含一个 `grad_fn` 属性，指向其计算来源，便于遍历计算图。

4. **梯度计算**：
   - 调用 `backward()` 方法时，自动微分框架会从当前张量开始，按照计算图的拓扑顺序，逐步计算各个节点的梯度。
   - 使用缓存和优化策略，提高梯度计算的效率。

### 示例：深入理解反向传播

以函数 $ f(x) = (x + 2)^2 $ 为例，手动推导其梯度和自动微分的计算过程。

#### 手动推导

1. **函数定义**：
   $ f(x) = (x + 2)^2 $

2. **导数计算**：
   $ f'(x) = 2(x + 2) $

#### 自动微分计算过程

1. **前向传播**：
   - $ a = x + 2 $
   - $ f(x) = a^2 $

2. **反向传播**：
   - $ \frac{\partial f}{\partial a} = 2a $
   - $ \frac{\partial a}{\partial x} = 1 $
   - 应用链式法则：
     $ \frac{\partial f}{\partial x} = \frac{\partial f}{\partial a} \cdot \frac{\partial a}{\partial x} = 2a \cdot 1 = 2(x + 2) $

#### 自动微分代码示例

```python
import touch

# 定义函数 f(x) = (x + 2)^2
def f(x):
    a = x + 2
    return a**2

# 初始化张量x，启用自动微分
x = touch.tensor(3.0, requires_grad=True)

# 计算函数值
y = f(x)

# 计算梯度
y.backward()

# 输出结果
print(f"f(x) = {y.data}, f'(x) = {x.grad.data}")
```

#### 运行结果

```
f(x) = 25.0, f'(x) = 10.0
```

手动计算导数 $ f'(3) = 2(3 + 2) = 10 $，与自动微分结果一致，验证了自动微分的准确性。

### 内部极值的理解

在梯度下降算法中，自动微分帮助我们找到函数的极值点（极小值或极大值）。通过计算梯度，我们可以确定函数在某一点的上升或下降趋势，并据此调整参数值，逐步逼近极值点。

以函数 $ f(x) = x^2 - 4x - 5 $ 为例，其极值点位于 $ x = 2 $，函数值为 -9。通过梯度下降，我们可以从任意初始点出发，沿着梯度的反方向更新参数，最终收敛到极值点。自动微分在这一过程中扮演了关键角色，通过高效准确地计算梯度，使得梯度下降算法能够快速收敛，找到函数的极值点。

## 结论

本次实验通过拍Touch框架，详细介绍了自动微分的基本概念、与手动微分的对比及其在梯度下降算法中的应用。通过具体的代码示例和深入的对比分析，我们掌握了如何定义函数、计算梯度、绘制函数图像以及实现梯度下降算法。自动微分极大地简化了复杂模型中的梯度计算过程，提高了开发效率和计算准确性。

### 关键要点总结

- **自动微分 vs 手动微分**：自动微分在处理复杂计算图时具有明显优势，能够高效、准确地计算梯度。
- **计算图机制**：自动微分通过构建计算图，系统记录每一步操作，支持反向传播算法计算梯度。
- **拍Touch的自动微分功能**：通过启用 `requires_grad`，自动记录计算图，使用 `backward()` 计算梯度，并通过优化器简化参数更新过程。
- **梯度下降算法**：自动微分简化了梯度计算和参数更新步骤，使得实现梯度下降算法更加便捷和高效。
- **验证与理解**：通过手动计算与自动微分的对比，验证了自动微分的准确性，并加深了对其内部机制的理解。
