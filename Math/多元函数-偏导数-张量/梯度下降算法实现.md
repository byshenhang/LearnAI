![image-20250112171215466](assets\image-20250112171215466.png)





![image-20250112171333462](assets\image-20250112171333462.png)



![image-20250112171403403](assets\image-20250112171403403.png)



![image-20250112171511727](assets\image-20250112171511727.png)



![image-20250112171540744](assets\image-20250112171540744.png)







# 梯度下降算法程序设计实验详细笔记

## 2. 函数与梯度

### 2.1 函数的变化率

在函数的某一点处，沿不同方向运动，函数值的变化率是不同的。梯度向量表示了函数在该点处各个方向的变化率，方向上升最快的方向即为梯度的方向。

### 2.2 梯度的定义

梯度（Gradient）定义为一个函数的所有偏导数构成的向量。对于单变量函数，梯度即为导数。

### 2.3 梯度的方向

梯度向量的方向是函数值变化率最大的方向。因此，若沿梯度的**反方向**移动，函数值将会减小。

## 3. 梯度下降算法概述

梯度下降算法的核心思想是通过迭代的方式，不断沿梯度的反方向更新自变量的值，使得目标函数逐步减小，最终逼近极小值点。

### 3.1 算法步骤

1. **初始化**：随机设置自变量 $ x $ 的初始值 $ x_0 $。
2. **计算梯度**：计算 $ f(x_0) $ 处的梯度 $ f'(x_0) $。
3. **更新 $ x $ 的值**：根据梯度的反方向，使用学习率 $ \alpha $ 更新 $ x $ 的值：
   $
   x_1 = x_0 - \alpha \cdot f'(x_0)
   $
4. **迭代**：将 $ x_1 $ 赋值给 $ x $，重复步骤 2 和 3，直到函数值收敛到最优解。

### 3.2 学习率 $ \alpha $

学习率 $ \alpha $ 控制每次迭代时 $ x $ 的更新步长。选择合适的 $ \alpha $ 能够保证算法的收敛性和效率。

![image-20250112171724686](assets\image-20250112171724686.png)

## 4. 示例：求解函数 $ f(x) = x^2 - 4x - 5 $ 的极小值

### 4.1 计算导数

给定一元函数：
$
f(x) = x^2 - 4x - 5
$
其导数为：
$
f'(x) = 2x - 4
$

### 4.2 梯度下降过程

- **极值点的性质**：函数 $ f(x) $ 是一个开口向上的抛物线（凸函数），其极小值即为全局最小值。通过求导数 $ f'(x) = 0 $，可得极小值点在 $ x = 2 $。

- **梯度下降迭代示例**：
  - **初始值**：设 $ x_0 = 5 $。
  - **第一步**：
    $
    f'(5) = 2 \times 5 - 4 = 6
    $
    更新 $ x $：
    $
    x_1 = 5 - \alpha \times 6
    $
    假设 $ \alpha = 0.001 $，则：
    $
    x_1 = 5 - 0.001 \times 6 = 4.994
    $
  - **第二步**：
    $
    f'(4.994) = 2 \times 4.994 - 4 = 5.988
    $
    更新 $ x $：
    $
    x_2 = 4.994 - 0.001 \times 5.988 = 4.988012
    $
  - **迭代过程**：继续上述步骤，$ x $ 将逐步逼近 $ 2 $。

### 4.3 梯度逐步减小

随着 $ x $ 趋近于极小值点 $ x = 2 $，梯度 $ f'(x) $ 逐渐减小，最终趋近于零。

## 5. 梯度下降算法代码实现

以下是利用梯度下降算法求解单变量函数 $ f(x) = x^2 - 4x - 5 $ 极小值的Python代码实现。

```python
def gradient(x):
    """
    计算函数 f(x) = x^2 - 4x - 5 的梯度 f'(x) = 2x - 4
    """
    return 2 * x - 4

def gradient_descent(x_init, alpha, iterations):
    """
    实现梯度下降算法

    参数：
    x_init: 初始x值
    alpha: 学习率
    iterations: 迭代次数

    返回：
    最终x值
    """
    x = x_init
    for i in range(iterations):
        grad = gradient(x)
        print(f"迭代次数: {i+1}, x值: {x:.6f}, 梯度: {grad:.6f}")
        x = x - alpha * grad
    return x

if __name__ == "__main__":
    # 初始化参数
    initial_x = 5.0        # 初始x值
    learning_rate = 0.001  # 学习率alpha
    num_iterations = 10    # 迭代次数

    # 执行梯度下降
    final_x = gradient_descent(initial_x, learning_rate, num_iterations)
    print(f"经过{num_iterations}次迭代后，x的最终值为: {final_x:.6f}")
```

### 5.1 代码说明

1. **梯度函数 `gradient(x)`**：
   - 计算给定 $ x $ 处的梯度 $ f'(x) = 2x - 4 $。

2. **梯度下降函数 `gradient_descent(x_init, alpha, iterations)`**：
   - **参数**：
     - `x_init`：自变量 $ x $ 的初始值。
     - `alpha`：学习率。
     - `iterations`：迭代次数。
   - **过程**：
     - 初始化 $ x $ 为 `x_init`。
     - 通过循环进行指定次数的迭代：
       - 计算当前 $ x $ 处的梯度 `grad`。
       - 打印当前迭代次数、$ x $ 的值及梯度值。
       - 更新 $ x $ 的值：$ x = x - \alpha \times \text{grad} $。
   - **返回**：
     - 迭代结束后，返回最终的 $ x $ 值。

3. **主函数 `if __name__ == "__main__":`**：
   - 设置初始参数：
     - `initial_x = 5.0`：初始 $ x $ 值。
     - `learning_rate = 0.001`：学习率 $ \alpha $。
     - `num_iterations = 10`：迭代次数。
   - 调用 `gradient_descent` 函数执行梯度下降，并输出最终的 $ x $ 值。

### 5.2 运行结果示例

假设执行上述代码，部分迭代结果如下：

```
迭代次数: 1, x值: 5.000000, 梯度: 6.000000
迭代次数: 2, x值: 4.994000, 梯度: 5.988000
迭代次数: 3, x值: 4.988012, 梯度: 5.976024
...
迭代次数: 10, x值: 4.954064, 梯度: 5.908128
经过10次迭代后，x的最终值为: 4.948156
```

从结果中可以观察到，随着迭代的进行，$ x $ 值逐步逼近极小值点 $ x = 2 $，梯度值逐渐减小。

### 5.3 增加迭代次数

若将迭代次数 `num_iterations` 设置为 10,000 次，可以使 $ x $ 更加接近 $ 2 $，从而更准确地求得函数的极小值。

