## **多元线性回归实验详细笔记**

---

## **一、多元线性回归的基本公式**
多元线性回归模型的假设函数为：
$
h_{\theta}(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \cdots + \theta_n x_n
$
其代价函数定义为：
$
J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)})^2
$
梯度下降公式：
$
\theta_j := \theta_j - \alpha \frac{\partial J(\theta)}{\partial \theta_j}
$
其中：

- $ \theta_j $ 是模型参数；
- $ \alpha $ 是学习率；
- $ m $ 是样本数量；
- $ n $ 是特征数量。

---

## **二、完整代码实现和注释**

### **1. 假设函数 `hypothesis`**

假设函数用来计算预测值 $ h_{\theta}(x) $，即根据输入样本特征和参数 $\theta$ 计算回归预测值。

```python
def hypothesis(theta, x, n):
    """
    计算假设函数的预测值 h_theta(x)。
    
    参数：
    - theta: 参数列表 [θ0, θ1, ..., θn]
    - x: 输入样本的特征向量 [x0, x1, ..., xn]
    - n: 特征数量 (不包括 x0)
    
    返回：
    - h: 预测值
    """
    h = 0  # 初始化预测值
    for i in range(n + 1):  # 遍历所有参数 θ 和对应特征 x
        h += theta[i] * x[i]  # 累加 θi * xi
    return h  # 返回最终的预测值
```

**注释解释**：
- 当 $ i = 0 $ 时，特征 $ x_0 = 1 $，对应 $ \theta_0 $，这相当于模型的偏置项。
- 循环累加所有参数与特征的乘积，最终得到预测值 $ h_{\theta}(x) $。

---

### **2. 偏导数计算函数 `gradient_partial_derivative`**

该函数用于计算代价函数 $ J(\theta) $ 对参数 $ \theta_j $ 的偏导数。

```python
def gradient_partial_derivative(X, Y, theta, n, m, j):
    """
    计算代价函数对 θ_j 的偏导数。
    
    参数：
    - X: 样本特征矩阵 (m 行 n 列)
    - Y: 样本标签向量 (m 个样本)
    - theta: 参数列表
    - n: 特征数量
    - m: 样本数量
    - j: 需要求偏导的参数索引
    
    返回：
    - 偏导数值
    """
    sum_val = 0  # 偏导数累加项
    for i in range(m):  # 遍历每个样本
        h = hypothesis(theta, X[i], n)  # 计算样本的预测值 h_theta(x)
        sum_val += (h - Y[i]) * X[i][j]  # 计算误差 * 对应特征 x_ij
    return sum_val / m  # 返回平均偏导数值
```

**注释解释**：
- 偏导数值衡量了代价函数在某个参数方向的变化程度，决定参数更新的方向和步长。
- 每个样本的预测误差 $(h - y)$ 与对应特征值相乘后累加，再除以样本数量 $ m $，得到平均偏导值。

---

### **3. 梯度下降函数 `gradient_descent`**

梯度下降法通过不断迭代更新参数，使得代价函数 $ J(\theta) $ 趋于最小。

```python
def gradient_descent(X, Y, n, m, alpha, iterate):
    """
    梯度下降迭代函数，用于更新模型参数 θ。
    
    参数：
    - X: 样本特征矩阵
    - Y: 样本标签向量
    - n: 特征数量
    - m: 样本数量
    - alpha: 学习率
    - iterate: 迭代次数
    
    返回：
    - theta: 更新后的参数列表
    """
    theta = [0] * (n + 1)  # 初始化参数 θ 为 0
    for _ in range(iterate):
        temp = theta.copy()  # 临时存储参数更新结果
        for j in range(n + 1):  # 更新每个 θ_j
            partial_derivative = gradient_partial_derivative(X, Y, theta, n, m, j)
            temp[j] -= alpha * partial_derivative  # 更新 θ_j
        theta = temp  # 更新参数列表
    return theta  # 返回最终参数 θ
```

**注释解释**：
- `temp` 列表用于同时更新所有参数，避免更新顺序导致计算错误。
- 通过 `alpha * 偏导数` 进行参数更新，`alpha` 越大，步长越大，但步长过大可能导致收敛失败。

---

### **4. 代价函数 `cost_function`**

代价函数用于评估模型的性能，计算所有样本的误差平方和的平均值。

```python
def cost_function(X, Y, theta, n, m):
    """
    计算代价函数 J(θ) 的值。
    
    参数：
    - X: 样本特征矩阵
    - Y: 样本标签向量
    - theta: 参数列表
    - n: 特征数量
    - m: 样本数量
    
    返回：
    - 代价函数值
    """
    sum_val = 0
    for i in range(m):
        h = hypothesis(theta, X[i], n)  # 计算样本预测值
        sum_val += (h - Y[i]) ** 2  # 计算误差平方
    return sum_val / (2 * m)  # 返回平均误差平方
```

---

### **5. 测试代码与实验数据**

```python
# 输入样本特征和标签
X = [
    [1, 2, 3, 4, 5],
    [1, 3, 4, 5, 6],
    [1, 4, 5, 6, 7],
    [1, 5, 6, 7, 8],
    [1, 6, 7, 8, 9],
    [1, 7, 8, 9, 10]
]
Y = [10, 15, 20, 25, 30, 35]

# 参数设置
n = 4  # 特征数量
m = 6  # 样本数量
alpha = 0.01  # 学习率
iterate = 1000  # 迭代次数

# 梯度下降训练模型
theta = gradient_descent(X, Y, n, m, alpha, iterate)
print(f"训练后的参数列表: {theta}")

# 打印代价函数值
cost = cost_function(X, Y, theta, n, m)
print(f"代价函数值: {cost}")

# 测试样本
test_samples = [
    [1, 8, 9, 10, 11],
    [1, 9, 10, 11, 12]
]
for i, sample in enumerate(test_samples):
    prediction = hypothesis(theta, sample, n)
    print(f"测试样本 {i+1} 的预测值: {prediction}")
```

---

## **三、多元线性回归与一元线性回归的对比**

| **比较项目**   | **一元线性回归**             | **多元线性回归**                                      |
| -------------- | ---------------------------- | ----------------------------------------------------- |
| **特征数量**   | 1 个特征                     | 多个特征                                              |
| **模型公式**   | $ y = \theta_0 + \theta_1x $ | $ y = \theta_0 + \theta_1x_1 + \cdots + \theta_nx_n $ |
| **参数数量**   | 2 个 (偏置和斜率)            | $ n+1 $ 个参数                                        |
| **输入数据**   | 一列特征值                   | 多列特征值                                            |
| **计算复杂度** | 计算量小                     | 计算量大                                              |

